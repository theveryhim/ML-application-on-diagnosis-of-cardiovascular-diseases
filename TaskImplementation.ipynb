{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning</h1>\n",
    "<h4 align=\"center\">Instructor : Dr. Shamsollahi</h4>\n",
    "<h4 align=\"center\">Electerical Engineering department of Sharif University of Technology, Spring 2024</h4>\n",
    "<h4 align=\"center\">Computer Assignment 2</h4>\n",
    "<h4 align=\"center\">Arman Yazdani 400102255</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis of cardiovascular diseases\n",
    "Cardiovascular diseases are the most important cause of death of Iranians, diseases which, according to the statistics of the Ministry of Health and Civil Registry, take the lives of at least 140,000 people every year and make thousands of people hospitalized, disabled and disabled. Diseases that depend more on lifestyle than any other factors such as nutrition, movement and exercise, smoking and air pollution and can be prevented to a large extent by controlling these factors. Our goal in this exercise is to use information \n",
    "People's health standard and different classification methods in machine learning to predict the existence of this category of diseases in people. The collection of data at your disposal \n",
    "It contains the information of 2000 patients. There are 18 columns in this data set, which are as follows from left to right:$\\\\$\n",
    "Column Names: ['age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'smoke', 'alco', 'active', 'cardio','cholesterol_1', 'cholesterol_2', 'cholesterol_3', 'gluc_1', 'gluc_2', 'gluc_3', 'bmi', 'bp']$\\newline$\n",
    "The variable 'cardio' is actually our label, which determines whether a person is sick (1 = cardio) or not (0 = cardio). In view of these cases \n",
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{1.\\hspace{0.2cm} Data-set\\quad preparation}\\newline$\n",
    "1.1:\n",
    "Read the dataset first and display the first five rows. Also, with the help of the $\\textit{.info()}$ command, display different information about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   age            1997 non-null   float64\n",
      " 1   gender         1997 non-null   float64\n",
      " 2   height         1991 non-null   float64\n",
      " 3   weight         1995 non-null   float64\n",
      " 4   ap_hi          1996 non-null   float64\n",
      " 5   ap_lo          1996 non-null   float64\n",
      " 6   smoke          1998 non-null   float64\n",
      " 7   alco           1995 non-null   float64\n",
      " 8   active         1996 non-null   float64\n",
      " 9   cardio         1996 non-null   float64\n",
      " 10  cholesterol_1  1994 non-null   float64\n",
      " 11  cholesterol_2  1991 non-null   float64\n",
      " 12  cholesterol_3  1996 non-null   float64\n",
      " 13  gluc_1         1993 non-null   float64\n",
      " 14  gluc_2         1996 non-null   float64\n",
      " 15  gluc_3         1993 non-null   float64\n",
      " 16  bmi            1997 non-null   float64\n",
      " 17  bp             1993 non-null   float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 281.4 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read CSV file\n",
    "data = pd.read_csv(\"cardio_pred.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  gender  height  weight  ap_hi\n",
      "0     51.0     1.0   168.0    62.0  110.0\n",
      "1     56.0     0.0   156.0    85.0  140.0\n",
      "2     52.0     0.0   165.0    64.0  130.0\n",
      "3     49.0     1.0   169.0    82.0  150.0\n",
      "4     49.0     0.0     NaN    56.0  100.0\n",
      "...    ...     ...     ...     ...    ...\n",
      "1995  51.0     0.0   160.0    58.0  120.0\n",
      "1996  40.0     1.0   176.0    75.0  160.0\n",
      "1997  46.0     0.0   164.0    74.0  120.0\n",
      "1998  55.0     1.0   164.0    75.0  120.0\n",
      "1999  63.0     0.0   155.0    72.0  160.0\n",
      "\n",
      "[2000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2: In this dataset, some of the cells are empty (missing values), remove the corresponding rows from this dataset. Then repeat the previous section.$\\\\$\n",
    "$\\textit{\\small{Check the indices on the left hand side}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  gender  height  weight  ap_hi\n",
      "0     51.0     1.0   168.0    62.0  110.0\n",
      "6     62.0     0.0   157.0    93.0  130.0\n",
      "7     63.0     1.0   178.0    95.0  130.0\n",
      "12    41.0     1.0   165.0    60.0  120.0\n",
      "14    40.0     1.0   181.0    95.0  130.0\n",
      "...    ...     ...     ...     ...    ...\n",
      "1995  51.0     0.0   160.0    58.0  120.0\n",
      "1996  40.0     1.0   176.0    75.0  160.0\n",
      "1997  46.0     0.0   164.0    74.0  120.0\n",
      "1998  55.0     1.0   164.0    75.0  120.0\n",
      "1999  63.0     0.0   155.0    72.0  160.0\n",
      "\n",
      "[1937 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "data1 = data.dropna()\n",
    "print(data1.iloc[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1937 entries, 0 to 1999\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   age            1937 non-null   float64\n",
      " 1   gender         1937 non-null   float64\n",
      " 2   height         1937 non-null   float64\n",
      " 3   weight         1937 non-null   float64\n",
      " 4   ap_hi          1937 non-null   float64\n",
      " 5   ap_lo          1937 non-null   float64\n",
      " 6   smoke          1937 non-null   float64\n",
      " 7   alco           1937 non-null   float64\n",
      " 8   active         1937 non-null   float64\n",
      " 9   cardio         1937 non-null   float64\n",
      " 10  cholesterol_1  1937 non-null   float64\n",
      " 11  cholesterol_2  1937 non-null   float64\n",
      " 12  cholesterol_3  1937 non-null   float64\n",
      " 13  gluc_1         1937 non-null   float64\n",
      " 14  gluc_2         1937 non-null   float64\n",
      " 15  gluc_3         1937 non-null   float64\n",
      " 16  bmi            1937 non-null   float64\n",
      " 17  bp             1937 non-null   float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 287.5 KB\n"
     ]
    }
   ],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3: Divide the data into a random 80% for train and 20% for test results.(set random state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dropping cardio\n",
    "X = data1.drop('cardio',axis=1)\n",
    "Y = data1['cardio']\n",
    "# Normalaizing (Z-Score) & Getting DataFrame\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "X_DataFrame = pd.DataFrame(X_norm, columns = X.columns)\n",
    "# Splitting Data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_DataFrame,Y,test_size=0.2,random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{2.\\hspace{0.2cm}Logistic\\hspace{0.2cm}Regression\\hspace{0.2cm}Classifier}$\n",
    "\n",
    "2.1:\n",
    "By training a logistic regression model, classify the data and get the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Perform Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Now we want to identify the most important features. Calculate the coefficients of the model defined in the previous section and explain the relationship between each feature and the output. Which features are more important and why؟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: ap_hi, Coefficient: 0.90\n",
      "Feature: age, Coefficient: 0.42\n",
      "Feature: cholesterol_3, Coefficient: 0.24\n",
      "Feature: height, Coefficient: 0.21\n",
      "Feature: bmi, Coefficient: 0.16\n",
      "Feature: ap_lo, Coefficient: 0.14\n",
      "Feature: gender, Coefficient: -0.14\n",
      "Feature: cholesterol_1, Coefficient: -0.14\n",
      "Feature: bp, Coefficient: 0.08\n",
      "Feature: gluc_2, Coefficient: 0.07\n",
      "Feature: gluc_3, Coefficient: -0.06\n",
      "Feature: cholesterol_2, Coefficient: -0.06\n",
      "Feature: smoke, Coefficient: -0.05\n",
      "Feature: active, Coefficient: -0.04\n",
      "Feature: alco, Coefficient: -0.03\n",
      "Feature: weight, Coefficient: -0.01\n",
      "Feature: gluc_1, Coefficient: -0.01\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients of the model\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Print the coefficients with feature names\n",
    "feature_names = X.columns\n",
    "# Create a list of tuples (feature_name, coefficient)\n",
    "coef_tuples = list(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the list of tuples by the absolute value of the coefficient\n",
    "coef_tuples.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the sorted coefficients\n",
    "for feature, coef in coef_tuples:\n",
    "    print(f\"Feature: {feature}, Coefficient: {coef:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ The sign of the coefficient (positive or negative) indicates the direction of the relationship between the feature and the target variable. A positive coefficient means that as the feature value increases, the likelihood of the target variable being 1 (or the positive class) increases. A negative coefficient means that as the feature value increases, the likelihood of the target variable being 1 decreases.$\\\\$\n",
    "$\\bullet$ The magnitude of the coefficient represents the strength of the relationship between the feature and the target variable. Features with larger absolute coefficients have a stronger impact on the prediction of the target variable.\n",
    "\n",
    "In the Cardiovascular Disease Prediction dataset, the feature ap_hi represents the systolic blood pressure of individuals. Systolic blood pressure is the higher value measured during a heartbeat and reflects the pressure in the arteries when the heart contracts.$\\\\$\n",
    "Elevated systolic blood pressure is a major risk factor for cardiovascular diseases which makes it the most important factor of Cardiovascular Disease Prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{3.Support\\hspace{0.1cm}Vector\\hspace{0.1cm}Machine\\hspace{0.1cm}Classifier}$\n",
    "\n",
    "3.1 This time، we are going to classify the dataset with SVM. First, define your model using the linear kernel and calculate the accuracy of different C-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for different C values:\n",
      "C = 0.1, Test accuracy: 0.74\n",
      "C = 1, Test accuracy: 0.74\n",
      "C = 10, Test accuracy: 0.75\n",
      "C = 100, Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "# Define the range of C values to test\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "# Create a dictionary of parameters for GridSearchCV\n",
    "param_grid = {'C': C_values}\n",
    "\n",
    "# Create the SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Use GridSearchCV to find the best C value\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the results for all C values\n",
    "print(\"Results for different C values:\")\n",
    "for c in C_values:\n",
    "    svm_model = SVC(kernel='linear', C=c)\n",
    "    svm_model.fit(X_train, Y_train)\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "    print(f\"C = {c}, Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Now use the poly kernel and calculate the accuracy of the classifier from different values of C and degree parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for different C and degree values:\n",
      "C = 0.1, Degree = 2, Test accuracy: 0.57\n",
      "C = 0.1, Degree = 3, Test accuracy: 0.68\n",
      "C = 0.1, Degree = 4, Test accuracy: 0.61\n",
      "C = 1, Degree = 2, Test accuracy: 0.60\n",
      "C = 1, Degree = 3, Test accuracy: 0.70\n",
      "C = 1, Degree = 4, Test accuracy: 0.66\n",
      "C = 10, Degree = 2, Test accuracy: 0.61\n",
      "C = 10, Degree = 3, Test accuracy: 0.70\n",
      "C = 10, Degree = 4, Test accuracy: 0.68\n",
      "C = 100, Degree = 2, Test accuracy: 0.61\n",
      "C = 100, Degree = 3, Test accuracy: 0.69\n",
      "C = 100, Degree = 4, Test accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Define the range of degree values to test\n",
    "degree_values = [2, 3, 4]\n",
    "\n",
    "# Create a dictionary of parameters for GridSearchCV\n",
    "param_grid = {'C': C_values, 'degree': degree_values}\n",
    "\n",
    "# Create the SVM model with a polynomial kernel\n",
    "svm_model = SVC(kernel='poly')\n",
    "\n",
    "# Use GridSearchCV to find the best C and degree values\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the results for all C and degree values\n",
    "print(\"Results for different C and degree values:\")\n",
    "for c in C_values:\n",
    "    for d in degree_values:\n",
    "        svm_model = SVC(kernel='poly', C=c, degree=d)\n",
    "        svm_model.fit(X_train, Y_train)\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "        print(f\"C = {c}, Degree = {d}, Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Finally, use the rbf kernel and calculate the accuracy of the classifier from different values of C and gamma parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for different C and gamma values:\n",
      "C = 0.1, Gamma = 0.01, Test accuracy: 0.74\n",
      "C = 0.1, Gamma = 0.1, Test accuracy: 0.71\n",
      "C = 0.1, Gamma = 1, Test accuracy: 0.52\n",
      "C = 0.1, Gamma = 10, Test accuracy: 0.51\n",
      "C = 1, Gamma = 0.01, Test accuracy: 0.74\n",
      "C = 1, Gamma = 0.1, Test accuracy: 0.71\n",
      "C = 1, Gamma = 1, Test accuracy: 0.64\n",
      "C = 1, Gamma = 10, Test accuracy: 0.52\n",
      "C = 10, Gamma = 0.01, Test accuracy: 0.73\n",
      "C = 10, Gamma = 0.1, Test accuracy: 0.70\n",
      "C = 10, Gamma = 1, Test accuracy: 0.61\n",
      "C = 10, Gamma = 10, Test accuracy: 0.54\n",
      "C = 100, Gamma = 0.01, Test accuracy: 0.72\n",
      "C = 100, Gamma = 0.1, Test accuracy: 0.67\n",
      "C = 100, Gamma = 1, Test accuracy: 0.60\n",
      "C = 100, Gamma = 10, Test accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "# Define the range of gamma values to test\n",
    "gamma_values = [0.01, 0.1, 1, 10]\n",
    "\n",
    "# Create a dictionary of parameters for GridSearchCV\n",
    "param_grid = {'C': C_values, 'gamma': gamma_values}\n",
    "\n",
    "# Create the SVM model with an RBF kernel\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "# Use GridSearchCV to find the best C and gamma values\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the results for all C and gamma values\n",
    "print(\"Results for different C and gamma values:\")\n",
    "for c in C_values:\n",
    "    for g in gamma_values:\n",
    "        svm_model = SVC(kernel='rbf', C=c, gamma=g)\n",
    "        svm_model.fit(X_train, Y_train)\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "        print(f\"C = {c}, Gamma = {g}, Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Compare the accuracy calculated in the previous sections. Which kernel performed best with what parameters؟ What is the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on part 2.2,the majority of the coefficients are small but one.which means training will result as an almost linear function of the biggest coefficient.\n",
    "later on,we observed that $\\\\$\n",
    "linear's best result was: $\\\\$\n",
    "C=10,100 => ACC=75% $\\\\$\n",
    "while Poly's was: $\\\\$\n",
    "C=0.1,degree=3 => ACC=70% $\\\\$\n",
    "and RBF's was: $\\\\$\n",
    "C=0.1,gamma=0.01 => ACC=74% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{4.Perceptron\\hspace{0.1cm}Classifier}$\n",
    "\n",
    "4.1 This time, with the help of perceptron algorithm, classify the data and get it's correctness.Do you use the sample mode algorithm or batch mode, why? (You are not allowed to use the Perceptron function in this section.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (batch mode): 0.49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('cardio_pred.csv')\n",
    "data = data.dropna()\n",
    "# Dropping cardio\n",
    "X = data.drop('cardio',axis=1)\n",
    "y = data['cardio']\n",
    "# Normalaizing (Z-Score) & Getting DataFrame\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "X_DataFrame = pd.DataFrame(X_norm, columns = X.columns)\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Implement the Perceptron algorithm in batch mode\n",
    "def perceptron_batch(X, y, epochs=100, learning_rate=0.1):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        mistakes = 0\n",
    "        for x, target in zip(X, y):\n",
    "            prediction = np.dot(x, weights) + bias\n",
    "            if np.sign(prediction) != target:\n",
    "                weights += learning_rate * target * x\n",
    "                bias += learning_rate * target\n",
    "                mistakes += 1\n",
    "        if mistakes == 0:\n",
    "            break\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Train the Perceptron model in batch mode\n",
    "weights, bias = perceptron_batch(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = np.sign(np.dot(X_test_scaled, weights) + bias)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test accuracy (batch mode): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5319244320135192\n",
      "Epoch 10, Loss: 0.1459605316894754\n",
      "Epoch 20, Loss: 0.317531674232084\n",
      "Epoch 30, Loss: 0.7981796544303684\n",
      "Epoch 40, Loss: 0.7629133422618124\n",
      "Epoch 50, Loss: 0.027593709582227857\n",
      "Epoch 60, Loss: 0.8927954537183166\n",
      "Epoch 70, Loss: 0.3364692997746001\n",
      "Epoch 80, Loss: 0.28944126602478476\n",
      "Epoch 90, Loss: 0.16608337290247377\n",
      "Epoch 100, Loss: 0.12433663918163967\n",
      "Epoch 110, Loss: 1.08808737945291\n",
      "Epoch 120, Loss: 0.3918185050231273\n",
      "Epoch 130, Loss: 0.1625598822075858\n",
      "Epoch 140, Loss: 0.013261025802569956\n",
      "Epoch 150, Loss: 0.13405288520744293\n",
      "Epoch 160, Loss: 0.8041216217303436\n",
      "Training Accuracy: 76.31%\n",
      "Test Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 17  # Number of input features\n",
    "hidden_size = 10  # Number of neurons in the hidden layer\n",
    "output_size = 1  # Binary classification output\n",
    "learning_rate = 0.01\n",
    "num_epochs = 170\n",
    "batch_size = 1\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('cardio_pred.csv')\n",
    "\n",
    "# Data Cleaning\n",
    "data = data.dropna()  # Drop rows with missing values\n",
    "\n",
    "# Features and Labels\n",
    "X = data.drop(columns=['cardio']).values\n",
    "y = data['cardio'].values.reshape(-1, 1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Activation Functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivatives\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Initialize Weights and Biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Forward and Backward Propagation\n",
    "def forward_backward_propagation(X, y):\n",
    "    global W1, b1, W2, b2\n",
    "\n",
    "    # Forward pass\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    # Compute loss (binary cross-entropy)\n",
    "    m = y.shape[0]\n",
    "    loss = -np.sum(y * np.log(A2) + (1 - y) * np.log(1 - A2)) / m\n",
    "\n",
    "    # Backward pass\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    indices = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "        loss = forward_backward_propagation(X_batch, y_batch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Model Evaluation\n",
    "def predict(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return (A2 > 0.5).astype(int)\n",
    "\n",
    "y_pred_train = predict(X_train)\n",
    "y_pred_test = predict(X_test)\n",
    "\n",
    "train_accuracy = np.mean(y_pred_train == y_train) * 100\n",
    "test_accuracy = np.mean(y_pred_test == y_test) * 100\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem that since we had total package of data,It's a better idea to train with batch mode but the results didn't do so.this could be due to outlayer data or skipping a regional extermum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 This time, with the help of the Perceptron Built-in Function classification and compare its accuracy with the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (built-in Perceptron): 0.64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('cardio_pred.csv')\n",
    "data = data.dropna()\n",
    "# Dropping cardio\n",
    "X = data.drop('cardio',axis=1)\n",
    "y = data['cardio']\n",
    "# Normalaizing (Z-Score) & Getting DataFrame\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "X_DataFrame = pd.DataFrame(X_norm, columns = X.columns)\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Use the built-in Perceptron function\n",
    "perceptron = Perceptron(random_state=42)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = perceptron.predict(X_test_scaled)\n",
    "accuracy_built_in = np.mean(y_pred == y_test)\n",
    "print(f\"Test accuracy (built-in Perceptron): {accuracy_built_in:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{5.MLP\\hspace{0.1cm}Neural\\hspace{0.1cm} Networks}$\n",
    "\n",
    "5.1 We are going to classify the data using neural networks (MLP). Research about different parameters of MLPClassifier (number of hidden neurons, activation functions, solver, etc.) and define your neural network using GridSearchCV and classify the data and report its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "Test accuracy: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HAMAHANG\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "# Create the MLPClassifier\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding test accuracy\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Test accuracy: {grid_search.score(X_test_scaled, y_test):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the best configuration found by the GridSearchCV is a neural network with two hidden layers, each with 100 neurons, using the logistic activation function, the Adam solver, a learning rate of 'constant', and an alpha (regularization) value of 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{6.Decision\\hspace{0.1cm}Tree}\\\\$\n",
    "\n",
    "6.1 In this section، we are going to classify the data with the help of the decision tree. First، find the best parameters of the tree (such as tree depth، impurity criteria). And, then define a DecisionTreeClassifier random_state model with = 2024 and classify the data using the parameters you have obtained, and report its accuracy. (Hint: You can use GridSearchCV.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2}\n",
      "Test accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "# Create the DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=2024)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding test accuracy\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Test accuracy: {grid_search.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Accuracies were:$\\\\$\n",
    "$\\textit{Logistic Regression}$ : 75% $\\\\$\n",
    "$\\textit{SVM(Linear)}$ : 75% $\\\\$\n",
    "$\\textit{SVM(Polynomial)}$ : 70% $\\\\$\n",
    "$\\textit{SVM(RBF)}$ : 74% $\\\\$\n",
    "$\\textit{Manual Perceptron(BS=1)}$ : 75% $\\\\$\n",
    "$\\textit{Manual Perceptron(BS=dataSize)}$ : 49% $\\\\$\n",
    "$\\textit{Built-in Perceptron}$ : 64% $\\\\$\n",
    "$\\textit{MLP}$ : 73% $\\\\$\n",
    "$\\textit{Decision Tree}$ : 68% $\\\\$\n",
    "where $\\textit{Logistic Regression,SVM(Linear),Perceptron}$ performed better which might be due to the nature of data(it is well established that MLP specially with higher hidden layers work better),parameter of model and etc.$\\\\$\n",
    "In addition to Acuracy we can estimate F1 Score,Percision,Least Squares Error and so on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
